{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:22:42.314360Z",
     "start_time": "2019-03-04T02:22:42.233109Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tools import load_data, save_prediction\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chandler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chandler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:22:45.157828Z",
     "start_time": "2019-03-04T02:22:45.129101Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!! Do not modify !!\n",
    "\"\"\"\n",
    "def dumb_featurize(text):\n",
    "    feats = {}\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    for word in words:\n",
    "        if word == \"love\" or word == \"like\" or word == \"best\":\n",
    "            feats[\"contains_positive_word\"] = 1\n",
    "        if word == \"hate\" or word == \"dislike\" or word == \"worst\" or word == \"awful\":\n",
    "            feats[\"contains_negative_word\"] = 1\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagofNGram_featurize(text):\n",
    "    \"\"\"\n",
    "    Bag of N-Grams Model\n",
    "    \"\"\"\n",
    "    bv = CountVectorizer(\n",
    "                        lowercase = True, \n",
    "                        analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer(),\n",
    "                        ngram_range=(1,2)\n",
    "                        )\n",
    "    listBvCount = bv.fit_transform([text])\n",
    "    vocab = bv.get_feature_names()\n",
    "    \n",
    "    return dict(zip(vocab,list(listBvCount.toarray()[0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfOneGram_featurize(text):\n",
    "    \"\"\"\n",
    "    Bag of words Model\n",
    "    \"\"\"\n",
    "    bv = CountVectorizer(\n",
    "                        lowercase = True, \n",
    "                        analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer(),\n",
    "                        min_df=0., max_df=1.\n",
    "                        )\n",
    "    listBvCount = bv.fit_transform([text])\n",
    "    vocab = bv.get_feature_names()\n",
    "    \n",
    "    return dict(zip(vocab,list(listBvCount.toarray()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfIDF_featurize(text):\n",
    "    \"\"\"\n",
    "    TF-IDF model\n",
    "    \"\"\"\n",
    "    tv = TfidfVectorizer(\n",
    "                        lowercase = True, \n",
    "                        analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer(),        \n",
    "                        min_df=0., max_df=1., \n",
    "                        use_idf=True\n",
    "                        )\n",
    "    listTvCount = tv.fit_transform([text])\n",
    "    vocab = tv.get_feature_names()\n",
    "    return dict(zip(vocab,list(np.round(listTvCount.toarray()[0],2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:01.246736Z",
     "start_time": "2019-03-04T02:36:01.215699Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import chain\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, feature_method=dumb_featurize, min_feature_ct=1, L2_reg=1.0):\n",
    "        \"\"\"\n",
    "        :param feature_method: featurize function\n",
    "        :param min_feature_count: int, ignore the features that appear less than this number to avoid overfitting\n",
    "        \"\"\"\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_ct = min_feature_ct\n",
    "        self.L2_reg = L2_reg\n",
    "\n",
    "    def featurize(self, X):\n",
    "        \"\"\"\n",
    "        # Featurize input text\n",
    "\n",
    "        :param X: list of texts\n",
    "        :return: list of featurized vectors\n",
    "        \"\"\"\n",
    "        featurized_data = []\n",
    "        for text in X:\n",
    "            # Removing stopwords and special character\n",
    "            for uselessWord in ['the','.',',','and','a','an',':','that','is','was']:\n",
    "                try:\n",
    "                    text.remove(uselessWord)\n",
    "                except:\n",
    "                    pass\n",
    "            feats = self.feature_method(text)\n",
    "            #print(feats)\n",
    "            featurized_data.append(feats)\n",
    "        #print(featurized_data)\n",
    "        return featurized_data\n",
    "\n",
    "    def pipeline(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to translate raw data input into sparse vectors\n",
    "        :param X: featurized input\n",
    "        :return: 2d sparse vectors\n",
    "        \n",
    "        Implement the pipeline method that translate the dictionary like feature vectors into homogeneous numerical\n",
    "        vectors, for example:\n",
    "        [{\"fea1\": 1, \"fea2\": 2}, \n",
    "         {\"fea2\": 2, \"fea3\": 3}] \n",
    "         --> \n",
    "         [[1, 2, 0], \n",
    "          [0, 2, 3]]\n",
    "          \n",
    "        Hints:\n",
    "        1. How can you know the length of the feature vector?\n",
    "        2. When should you use sparse matrix?\n",
    "        3. Have you treated non-seen features properly?\n",
    "        4. Should you treat training and testing data differently?\n",
    "        \"\"\"\n",
    "        # Have to build feature_vocab during training\n",
    "        if training:\n",
    "            finalOutput = []\n",
    "            # get the full feature vector\n",
    "            #listFull = list(set(chain.from_iterable(X)))\n",
    "            for ls in X:\n",
    "                self.feature_vocab = dict(self.feature_vocab, **ls)\n",
    "            # translate the dictionary like feature vectors into homogeneous numerical vectors\n",
    "            for ls in X:\n",
    "                output = []\n",
    "                for vector in self.feature_vocab.keys():\n",
    "                    if vector in list(ls.keys()):\n",
    "                        output.append(ls[vector])\n",
    "                    else:\n",
    "                        output.append(0)\n",
    "                finalOutput.append(output) \n",
    "                \n",
    "            return np.array(finalOutput)        \n",
    "            \n",
    "            #raise NotImplementedError\n",
    "         \n",
    "        # Translate raw texts into vectors\n",
    "        else:\n",
    "            finalOutput = []\n",
    "            # use same full feature vector from training data\n",
    "            # translate the dictionary like feature vectors into homogeneous numerical vectors\n",
    "            for ls in X:\n",
    "                output = []\n",
    "                for vector in self.feature_vocab.keys():\n",
    "                    if vector in list(ls.keys()):\n",
    "                        output.append(ls[vector])\n",
    "                    else:\n",
    "                        output.append(0)\n",
    "                finalOutput.append(output) \n",
    "            return np.array(finalOutput) \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X), training=True)\n",
    "        #print(X)\n",
    "        D, F = X.shape\n",
    "        self.model = LogisticRegression(C=self.L2_reg)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "    # Write learned parameters to file\n",
    "    def save_weights(self, filename='weights.csv'):\n",
    "        weights = [[\"__intercept__\", self.model.intercept_[0]]]\n",
    "        for feat, idx in self.feature_vocab.items():\n",
    "            weights.append([feat, self.model.coef_[0][idx]])\n",
    "        \n",
    "        weights = pd.DataFrame(weights)\n",
    "        weights.to_csv(filename, header=False, index=False)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:02.841682Z",
     "start_time": "2019-03-04T02:36:02.812608Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this to test your model implementation\n",
    "\"\"\"\n",
    "\n",
    "cls = SentimentClassifier()\n",
    "X_train = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "\n",
    "X = cls.pipeline(X_train, True)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize training features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "X = cls.pipeline(X_test)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize testing features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea4\": 3}]\n",
    "try:\n",
    "    X = cls.pipeline(X_test)\n",
    "    assert X.shape[0] == 2 and X.shape[1] >= 3\n",
    "except:\n",
    "    print(\"Fail to treat un-seen features\")\n",
    "    raise Exception\n",
    "    \n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:36:14.338290Z",
     "start_time": "2019-03-04T02:36:13.670195Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.5409429280397022\n",
      "Dev set accuracy:  0.5057803468208093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chandler\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model performance - dumb method\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train_sample.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "cls = SentimentClassifier(feature_method=dumb_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model performance\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Bag of N Grams\n",
    "cls = SentimentClassifier(feature_method=bagofNGram_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy using bag of N Grams: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))\n",
    "save_prediction(cls.predict(X_dev),filename=\"NGram_prediction_withLemma.csv\")\n",
    " \n",
    "# Bag of one word\n",
    "cls = SentimentClassifier(feature_method=bagOfOneGram_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy using bag of words: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))\n",
    "save_prediction(cls.predict(X_dev),filename=\"bagWords_prediction_withLemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model performance\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# TF-IDF Model\n",
    "\n",
    "cls = SentimentClassifier(feature_method=tfIDF_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy using IF-IDF: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))\n",
    "save_prediction(cls.predict(X_dev),filename=\"TF_IDF_prediction_withLemma.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T02:10:36.091653Z",
     "start_time": "2019-03-04T02:10:35.890808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to save weights and the prediction\n",
    "\"\"\"\n",
    "weights = cls.save_weights()\n",
    "\n",
    "X_test = load_data(\"test.txt\").text\n",
    "save_prediction(cls.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Use different learning methods\n",
    "\n",
    "God job reaching this point! So far you have explored many different ways of doing feature engineering, but how about the learning method? In the previous implementation Logistic Regression was used. Now you can try to use different learning methods.\n",
    "\n",
    "hint: inherit the previous model and overwrite the `fit` method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Ref:\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
