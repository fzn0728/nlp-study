{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will learn:\n",
    "    - Forward propagation of a CNN network\n",
    "    - Backward propagation of a CNN network\n",
    "    - Numerical gradient checking \n",
    "    - Use Keras and TensorFlow to implement more complex CNN networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:05:15.843496Z",
     "start_time": "2019-03-20T05:05:15.769561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tools import load_data, read_vocab, sigmoid, tanh, show_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer, word_tokenize,download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:40:29.169465Z",
     "start_time": "2019-03-18T04:40:29.086590Z"
    }
   },
   "source": [
    "# CNN model \n",
    "Complete the code block in the cells in this section.\n",
    "\n",
    "* step1: Implement the pipeline method to process the raw input\n",
    "* step2: Implement the forward method\n",
    "* step3: Implement the backward method\n",
    "* step4: Run the cell below to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T07:00:18.632501Z",
     "start_time": "2019-03-19T06:47:59.260497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 1\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 2\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 3\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 4\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 5\tTrain accuracy: 0.490\tDev accuracy: 0.506\n",
      "Epoch: 6\tTrain accuracy: 0.490\tDev accuracy: 0.506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-80d187e14bfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNNTextClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnEpoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-2d2624f0df93>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_dev, y_dev, nEpoch)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0maccuracy_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-2d2624f0df93>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prob\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-2d2624f0df93>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_indices)\u001b[0m\n\u001b[0;32m    132\u001b[0m                     \u001b[0mwindow_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_indices_ind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwindow_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mF_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mtemp_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mF_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0mhid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mF_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell shows you how the model will be used, you have to finish the cell below before you\n",
    "can run this cell. \n",
    "\n",
    "Once the implementation is done, you should hype tune the parameters to find the best config\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_data(\"train.txt\")\n",
    "vocab = read_vocab(\"vocab.txt\")\n",
    "\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3) \n",
    "cls = CNNTextClassificationModel(vocab)\n",
    "cls.train(X_train, y_train, X_dev, y_dev, nEpoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('f', 'f', 'f')\n",
      "1\n",
      "1 ('l', 'l', 'l')\n",
      "1\n",
      "2 ('o', 'o', 'i')\n",
      "2\n",
      "3 ('w', 'w', 'g')\n",
      "2\n",
      "fl\n",
      "flower flow flight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'flight'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [\"flower\",\"flow\",\"flight\"]\n",
    "for i, string in enumerate(zip(*temp)):\n",
    "    print(i, string)\n",
    "    print(len(set(string)))\n",
    "print(temp[0][:2])\n",
    "print(*temp)\n",
    "min(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(window([1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T07:21:55.709144Z",
     "start_time": "2019-03-19T07:21:55.618177Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CNNTextClassificationModel:\n",
    "    def __init__(self, vocab, window_size=2, F=100, alpha=0.1):\n",
    "        \"\"\"\n",
    "        F: number of filters\n",
    "        alpha: back propagatoin learning rate\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.F = F\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # U and w are the weights of the hidden layer, see Fig 1 in the pdf file\n",
    "        # U is the 1D convolutional layer with shape: voc_size * num_filter * window_size\n",
    "        self.U = np.random.normal(loc=0, scale=0.01, size=(len(vocab), F, window_size))\n",
    "        # w is the weights of the activation layer (after max pooling)\n",
    "        self.w = np.random.normal(loc=0, scale=0.01, size=(F + 1))\n",
    "        \n",
    "    def pipeline(self, X):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to:\n",
    "        1. Tokenize, Normalize the raw input\n",
    "        2. Translate raw data input into numerical encoded vectors\n",
    "        \n",
    "        :param X: raw data input\n",
    "        :return: list of lists\n",
    "        \n",
    "        For example:\n",
    "        X = [\"Apples orange banana\",\n",
    "         \"orange apple bananas\"] \n",
    "        returns:\n",
    "        [[0, 1, 2, \n",
    "         1, 0, 2]]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        X2 = []\n",
    "        unknown = vocab['__unknown__']\n",
    "        default = vocab['.']\n",
    "        wnet = WordNetLemmatizer()\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            cleaned_tokens = [self.vocab.get(wnet.lemmatize(w), unknown) for w in word_tokenize(X[i])]\n",
    "            if len(cleaned_tokens) < self.window_size:\n",
    "                cleaned_tokens = cleaned_tokens + [default] * (self.window_size - len(cleaned_tokens))\n",
    "            X2.append(cleaned_tokens)\n",
    "\n",
    "        return X2\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(probs, labels):\n",
    "        assert len(probs) == len(labels), \"Wrong input!!\"\n",
    "        a = np.array(probs)\n",
    "        b = np.array(labels)\n",
    "        \n",
    "        return 1.0 * (a==b).sum() / len(b) \n",
    "          \n",
    "    def train(self, X_train, y_train, X_dev, y_dev, nEpoch=50):\n",
    "        \"\"\"\n",
    "        Function to fit the model\n",
    "        :param X_train, X_dev: raw data input\n",
    "        :param y_train, y_dev: label \n",
    "        :nEpoch: number of training epoches\n",
    "        \"\"\"\n",
    "        X_train = self.pipeline(X_train)\n",
    "        X_dev = self.pipeline(X_dev)\n",
    "        #print(X_train.shape)\n",
    "        #print(X_train[0])\n",
    "        for epoch in range(nEpoch):\n",
    "            self.fit(X_train, y_train)\n",
    "            \n",
    "            accuracy_train = self.accuracy(self.predict(X_train), y_train)\n",
    "            accuracy_dev = self.accuracy(self.predict(X_dev), y_dev)\n",
    "            \n",
    "            print(\"Epoch: {}\\tTrain accuracy: {:.3f}\\tDev accuracy: {:.3f}\"\n",
    "                  .format(epoch, accuracy_train, accuracy_dev))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: numerical encoded input\n",
    "        \"\"\"\n",
    "        for (data, label) in zip(X, y):\n",
    "            self.backward(data, label)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: numerical encoded input\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for data in X:\n",
    "            if self.forward(data)[\"prob\"] > 0.5:\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def forward(self, word_indices):\n",
    "        \"\"\"\n",
    "        :param word_indices: a list of numerically ecoded words\n",
    "        :return: a result dictionary containing 3 items -\n",
    "        result['prob']: \\hat y in Fig 1.\n",
    "        result['h']: the hidden layer output after max pooling, h = [h1, ..., hf]\n",
    "        result['hid']: argmax of F filters, e.g. j of x_j\n",
    "        e.g. for the ith filter u_i, tanh(word[hid[j], hid[j] + width]*u_i) = h_i\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(word_indices) >= self.window_size, \"Input length cannot be shorter than the window size\"\n",
    "\n",
    "        h = np.zeros(self.F + 1, dtype=float)\n",
    "        hid = np.zeros(self.F, dtype=int)\n",
    "        prob = 0.0\n",
    "\n",
    "        # layer 1. compute h and hid\n",
    "        # loop through the input data of word indices and\n",
    "        # keep track of the max filtered value h_i and its position index x_j\n",
    "        # h_i = max(tanh(weighted sum of all words in a given window)) over all windows for u_i\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        #print(word_indices[0])\n",
    "        for F_index in range(len(self.U[0])):\n",
    "            temp_list = []\n",
    "            for word_indices_ind in range(len(word_indices) - self.window_size + 1):\n",
    "                window_sum = 0.0\n",
    "                for window_ind in range(self.window_size):\n",
    "                    window_sum += self.U[word_indices[word_indices_ind + window_ind]][F_index][window_ind]\n",
    "                temp_list.append(tanh(window_sum))\n",
    "            h[F_index] = np.max(temp_list)\n",
    "            hid[F_index] = np.argmax(temp_list)\n",
    "        h[-1] = 1\n",
    "\n",
    "        # layer 2. compute probability\n",
    "        # once h and hid are computed, compute the probabiliy by sigmoid(h^TV)\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        prob_sum = 0.0\n",
    "        for w_i, h_i in zip(self.w, h):\n",
    "            prob_sum += w_i * h_i\n",
    "\n",
    "        prob = sigmoid(prob_sum)\n",
    "        # return result\n",
    "        return {\"prob\": prob, \"h\": h, \"hid\": hid}\n",
    "    \n",
    "    def backward(self, word_indices, label):\n",
    "        \"\"\"\n",
    "        Update the U, w using backward propagation\n",
    "        \n",
    "        :param word_indices: a list of numerically ecoded words\n",
    "        :param label: int 0 or 1\n",
    "        :return: None\n",
    "        \n",
    "        update weight matrix/vector U and V based on the loss function\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(word_indices) >= self.window_size, \"Input length cannot be shorter than the window size\"\n",
    "\n",
    "        h = np.zeros(self.F + 1, dtype=float)\n",
    "        hid = np.zeros(self.F, dtype=int)\n",
    "        prob = 0.0\n",
    "\n",
    "        # layer 1. compute h and hid\n",
    "        # loop through the input data of word indices and\n",
    "        # keep track of the max filtered value h_i and its position index x_j\n",
    "        # h_i = max(tanh(weighted sum of all words in a given window)) over all windows for u_i\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        #print(word_indices[0])\n",
    "        for F_index in range(len(self.U[0])):\n",
    "            temp_list = []\n",
    "            for word_indices_ind in range(len(word_indices) - self.window_size + 1):\n",
    "                window_sum = 0.0\n",
    "                for window_ind in range(self.window_size):\n",
    "                    window_sum += self.U[word_indices[word_indices_ind + window_ind]][F_index][window_ind]\n",
    "                    \n",
    "                #print(word_indices_ind, self.U[word_indices[word_indices_ind+window_ind]][F_index][window_ind])\n",
    "                temp_list.append(tanh(window_sum))\n",
    "            h[F_index] = np.max(temp_list)\n",
    "            hid[F_index] = np.argmax(temp_list)\n",
    "        h[-1] = 1\n",
    "\n",
    "        # layer 2. compute probability\n",
    "        # once h and hid are computed, compute the probabiliy by sigmoid(h^TV)\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        prob_sum = 0.0\n",
    "        for w_i, h_i in zip(self.w, h):\n",
    "            prob_sum += w_i * h_i\n",
    "\n",
    "        prob = sigmoid(prob_sum)\n",
    "        # return result\n",
    "        return {\"prob\": prob, \"h\": h, \"hid\": hid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Build your model using Keras + Tensorflow\n",
    "\n",
    "So far we have always forced you to implement things from scratch. You may feel it's overwhelming, but fortunately, it is not how the real world works. In the real world, there are existing tools you can leverage, so you can focus on the most innovative part of your work. We asked you to do all the previous execises for learning purpose, and since you have already reached so far, it's time to unleash yourself and allow you the access to the real world toolings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:06:30.995290Z",
     "start_time": "2019-03-20T05:06:30.927192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's see how you can build a similar CNN model you just had using Keras\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:03:13.785839Z",
     "start_time": "2019-03-20T05:03:13.732270Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yes! it is a good practice to do data processing outside the ML model\n",
    "wnet = WordNetLemmatizer()\n",
    "# Numerical encode all the words\n",
    "unknown = vocab['__unknown__']\n",
    "X_train2 = [[vocab.get(wnet.lemmatize(w), unknown) for w in word_tokenize(sent)] for sent in X_train]\n",
    "X_dev2 = [[vocab.get(wnet.lemmatize(w), unknown)for w in word_tokenize(sent)] for sent in X_dev]\n",
    "\n",
    "# Tensorflow does not handle variable length input well, let's unify all input to the same length\n",
    "def trim_X(X, max_length=100, default=vocab['.']):\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i]) > max_length:\n",
    "            X[i] = X[i][:max_length]\n",
    "        elif len(X[i]) < max_length:\n",
    "            X[i] = X[i] + [default] * (max_length - len(X[i]))\n",
    "            \n",
    "    return np.array(X)\n",
    "            \n",
    "X_train2 = trim_X(X_train2, MAX_LENGTH)\n",
    "X_dev2 = trim_X(X_dev2, MAX_LENGTH)\n",
    "\n",
    "\n",
    "# Now we have all the input data nicely encoded with numerical label, and each of the input data are trimmed \n",
    "# to have the same length. We would have needed to further apply one-hot encode for each word. However, this \n",
    "# would be very expensive, since each word will be expanded into a len(vocab) (~10000) length vector. Keras does\n",
    "# not support sparse matrix input at this moment. But don't worry, we will use an advanced technique called embedding\n",
    "# layer. This concept will be introduced in the next lesson. At this moment, you don't have to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:38:00.091414Z",
     "start_time": "2019-03-20T05:37:59.875258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(vocab), input_length=MAX_LENGTH, output_dim=512, name=\"Embedding-1\"))\n",
    "model.add(Conv1D(filters=100, kernel_size=2, activation=\"tanh\", name=\"Conv1D-1\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"MaxPooling1D-1\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"Dense-1\"))\n",
    "print(model.summary())\n",
    "\n",
    "show_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:39:27.542489Z",
     "start_time": "2019-03-20T05:38:02.612896Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train, epochs=10, validation_data=[X_dev2, y_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown you have to use an industry level tool to build a CNN model. Hopefully you think it is simpler than the version we built from scratch. Not really? Read Keras Documentation and learn more: https://keras.io/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T05:40:28.362651Z",
     "start_time": "2019-03-20T05:40:28.304975Z"
    },
    "collapsed": true
   },
   "source": [
    "# # Now it's your turn to build some more complicated CNN models\n",
    "\n",
    "\"\"\"\n",
    "Implement your code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "### https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "### https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
